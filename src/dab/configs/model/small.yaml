vocab_size: 32
padding_idx: 1

# 19M parameters (if no chain-aware attention)
# 24M parameters (if chain-aware attention)
d_model: 256
n_layers: 24
n_heads: 4
d_ffn: null
ffn_multiplier: null

max_seq_len: 320
max_timesteps: 100
use_timestep_embedding: false

dropout: 0.1
attention_dropout: 0.1
embedding_dropout: 0.1

use_chain_aware_attention: true
