vocab_size: 32
padding_idx: 1

# 99M parameters (if no chain-aware attention)
# 124M parameters (if chain-aware attention)
d_model: 384
n_layers: 56
n_heads: 6
d_ffn: null
ffn_multiplier: null

max_seq_len: 320
max_timesteps: 100
use_timestep_embedding: false

dropout: 0.1
attention_dropout: 0.1
embedding_dropout: 0.1

use_chain_aware_attention: true
